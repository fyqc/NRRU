import hashlib
import json
import logging
import os
import shutil
import subprocess
import re
import requests
import time
from bs4 import BeautifulSoup
from PIL import Image
from threading import Thread


"""
ç”¨äºå¾®ä¿¡å…¬ä¼—å·ä¸­çš„è¯¸å¤šå›¾ç‰‡ä¿å­˜

ä»¥æ–‡ç« æ ‡é¢˜ä¸ºæ–‡ä»¶å¤¹çš„åå­—
ä»¥å›¾ç‰‡æœ¬èº«çš„MD5ä½œä¸ºæ–‡ä»¶å

å°†WebPè‡ªåŠ¨è½¬æ¢ä¸ºPNG

æ“ä½œè¯´æ˜ï¼š
ä»icloudé‚£é‡ŒæŠŠURLå¤åˆ¶åˆ°txtæ–‡ä»¶ä¸­
ç„¶åæ‰§è¡Œï¼Œå°±å¯ä»¥åœ¨ç›®æ ‡æ–‡ä»¶å¤¹ä¸­ç”Ÿæˆä»¥æ ‡é¢˜ä¸ºæ–‡ä»¶å¤¹åçš„æ–‡ä»¶å¤¹ï¼Œé‡Œé¢æ˜¯å¸–å­ä¸­çš„å›¾ç‰‡
"""

# 2024å¹´11æœˆ4æ—¥
# é‡æ–°å¯ç”¨æ•°æ®åº“
# ä¸å†å°†é‡å¤æ–‡ä»¶ç§»å…¥ä¸´æ—¶æ–‡ä»¶å¤¹ï¼Œç›´æ¥åˆ é™¤
# å°†åŸå…ˆçš„åˆ—è¡¨æ”¹è¿›ä¸ºtxtæ–‡æ¡£
# å‘ç°æ¼ä¸‹äº†ä¸€äº›å›¾ç‰‡ï¼Œé‚ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä¸€æŠŠæ¢­

logging.basicConfig(
    format='%(asctime)s - %(levelname)s - %(message)s',
    # level=logging.DEBUG,
    level=logging.INFO,
    handlers=[logging.FileHandler(
        filename='tencent.log', mode='w', encoding='utf-8')]
)

"""
æ—¥å¿—çº§åˆ«     ä½¿ç”¨åœºæ™¯
DEBUG 	    æœ€ä½çº§åˆ«ã€‚ç”¨äºå°ç»†èŠ‚ã€‚é€šå¸¸åªæœ‰åœ¨è¯Šæ–­é—®é¢˜æ—¶ï¼Œä½ æ‰ä¼šå…³å¿ƒè¿™äº›æ¶ˆæ¯
INFO 	    ç”¨äºè®°å½•ç¨‹åºä¸­ä¸€èˆ¬äº‹ä»¶çš„ä¿¡æ¯ï¼Œæˆ–ç¡®è®¤ä¸€åˆ‡å·¥ä½œæ­£å¸¸
WARNING 	ç”¨äºè¡¨ç¤ºå¯èƒ½çš„é—®é¢˜ï¼Œå®ƒä¸ä¼šé˜»æ­¢ç¨‹åºçš„å·¥ä½œï¼Œä½†å°†æ¥å¯èƒ½ä¼š
ERROR 	    ç”¨äºè®°å½•é”™è¯¯ï¼Œå®ƒå¯¼è‡´ç¨‹åºåšæŸäº‹å¤±è´¥
CRITICAL 	æœ€é«˜çº§åˆ«ã€‚ç”¨äºè¡¨ç¤ºè‡´å‘½çš„é”™è¯¯ï¼Œå®ƒå¯¼è‡´æˆ–å°†è¦å¯¼è‡´ç¨‹åºå®Œå…¨åœæ­¢å·¥ä½œ

æ—¥å¿—çº§åˆ«é‡è¦ç¨‹åº¦é€æ¬¡æé«˜ï¼Œloggingåˆ†åˆ«æä¾›äº†5ä¸ªå¯¹åº”çº§åˆ«çš„æ–¹æ³•ã€‚é»˜è®¤æƒ…å†µä¸‹æ—¥å¿—çš„çº§åˆ«æ˜¯WARGINGï¼Œ ä½äºWARINGçš„æ—¥å¿—ä¿¡æ¯éƒ½ä¸ä¼šè¾“å‡ºã€‚
ä»æœ€ä¸é‡è¦åˆ°æœ€é‡è¦ã€‚åˆ©ç”¨ä¸åŒçš„æ—¥å¿—å‡½æ•°ï¼Œæ¶ˆæ¯å¯ä»¥æŒ‰æŸä¸ªçº§åˆ«è®°å…¥æ—¥å¿—ã€‚
"""


HEADER = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36'
    ' (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.70',
    'Accept-Language': 'en-US,en;q=0.5',
}

STATIC = r"F:\Lab\libwebp-1.4.0-windows-x64\bin\dwebp.exe"
SAVE_DIRECTORY = r"D:\RMT\Tencent Wechat\tecent public"
DATABSE = r"D:\RMT\Tencent Wechat\Wechat\wc.json"
ICLOUD = r"D:\RMT\Tencent Wechat\Wechat\download.txt"


def get_urls_from_download_txt(ICLOUD):
    urls = []
    with open(ICLOUD, 'r') as f:
        content = f.readlines()
        for line in content:
            if line.startswith('https'):
                urls.append(line.strip('\n'))
    return urls


def get_soup_from_localhtml(webpage):
    with open(webpage, 'r', encoding='utf-8') as file:
        content = file.read()
    soup = BeautifulSoup(content, features='lxml')

    return soup, content


def get_soup_from_webpage(url: str, header: str, timeout=None):
    attempts = 0
    success = False
    while attempts < 6 and not success:
        try:
            response = requests.get(url, headers=header, timeout=15)
            if response.status_code == requests.codes.ok:  # ok means 200 only

                content = response.text
                soup = BeautifulSoup(content, 'lxml')
                success = True
            else:
                print(f"{url} ä¸‹è½½å¤±è´¥ çŠ¶æ€ç ï¼š {response.status_code}")
                logging.error(f"{url} ä¸‹è½½å¤±è´¥ çŠ¶æ€ç ï¼š {response.status_code}")
                break
        except:
            print(f"{url} \t è¿æ¥é”™è¯¯ï¼Œæ— æ³•åŠ è½½\n")
            # ä»¥ç­‰å·®æ•°åˆ—æ·»åŠ ç­‰å¾…æ—¶é—´é—´éš”
            print(
                f'å°è¯•æ‰“å¼€ {url} æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯ï¼Œç­‰å¾…{5*attempts + 5}ç§’é’Ÿåè¿›è¡Œç¬¬{attempts + 1}æ¬¡å°è¯•')
            time.sleep(5*attempts + 5)
            attempts += 1

    return soup, content


def make_name_valid(validname: str) -> str:
    validname = validname.replace('\\', '_')
    validname = validname.replace('/', '_')
    validname = validname.replace(':', '_')
    validname = validname.replace('*', '_')
    validname = validname.replace('?', '_')
    validname = validname.replace('"', '_')
    validname = validname.replace('<', '_')
    validname = validname.replace('>', '_')
    validname = validname.replace('|', '_')
    validname = validname.replace('|', '_')
    validname = validname.replace('\t', '_')
    validname = validname.replace('\r', '_')
    validname = validname.replace('\n', '_')
    # \xa0 Unicode represents a hard space or a no-break space in a program.
    validname = validname.replace('\xa0', '')
    validname = validname.replace('ï½', '')
    validname = validname.replace('ğŸ“·', '').replace('ğŸˆ', '').replace('ğŸ˜œ', '')
    # åœ¨ Windows ç³»ç»Ÿä¸­å»ºç«‹æ–‡ä»¶å¤¹æ—¶åå­—çš„æœ€åä¸èƒ½æ˜¯â€œï¼â€ï¼Œä¸è®ºä½ åŠ å¤šå°‘ä¸ªç‚¹ï¼Œéƒ½ä¼šè¢« Windows å¿½ç•¥ã€‚
    validname = validname.rstrip(".")
    validname = validname.strip()
    return validname


def extract_image_using_re_only(content: str) -> list:
    pattern = r"(?:['\"]?cdn_url['\"]?)\s*:\s*['\"]https?://mmbiz[^\s\"']*['\"]"

    raw_urls = re.findall(pattern, content)

    logging.debug(raw_urls)

    if raw_urls:
        pure_urls = [url.replace(" ", "").split(
            ":'")[-1].split("\\")[0] for url in raw_urls]

        # å»é‡
        downlist = list(set(pure_urls))

        return downlist

    else:
        # æ—©æœŸé“¾æ¥å½¢å¼ä¸º
        # data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/MVPvEL7Qg0G6NN3oSIm4CuPDWGQo3LX6fRzicLvdbmssAa1qwOibrXjeNicbziaca2K5RQFZg6X5NdUVXYvK9rkiaBA/640?wx_fmt=png"
        pattern = r"(?:data-src=['\"]?)\s*['\"]https?://mmbiz[^\s\"']*['\"]"

        raw_urls = re.findall(pattern, content)

        print("å‘ç°å¤æ—©é“¾æ¥å½¢å¼ï¼Œä½¿ç”¨åå¤‡è§£ææ–¹æ³•ã€‚")
        logging.info("å‘ç°å¤æ—©é“¾æ¥å½¢å¼ï¼Œä½¿ç”¨åå¤‡è§£ææ–¹æ³•ã€‚")
        logging.debug(f"2nd raw_urls:\n{raw_urls}")

        if raw_urls:
            pure_urls = [url.split("data-src=")[-1].replace('"', '')
                         for url in raw_urls]

            # å»é‡
            downlist = list(set(pure_urls))

            return downlist

        else:
            print("No image URLs found.")
            return None


def find_title(url: str, soup: BeautifulSoup) -> str:
    tag_title = soup.find(
        'h1', class_='rich_media_title')

    if tag_title:
        return tag_title.get_text().strip()
    else:
        meta_web_title = soup.find("meta", property="og:title")

    if meta_web_title:
        return meta_web_title['content']
    else:
        tag_h2 = soup.find(
            'h2', class_='weui-msg__title title')

    if tag_h2:
        return tag_h2.get_text()
    else:
        tag_div = soup.find(
            'div', class_='weui-msg__title warn'
        )

    if tag_div:
        return tag_div.get_text()
    else:
        tag_script = soup.find('script')

    if tag_script:
        script_content = tag_script.string
        if 'Weixin Official Accounts Platform' in script_content:
            return 'Weixin Official Accounts Platform'
        else:
            'é©å‘½å°šæœªæˆåŠŸ åŒå¿—ä»éœ€åŠªåŠ› ï¼ˆæœªèƒ½è¯†åˆ«æ ‡é¢˜ï¼Œè¯·ä¿®æ”¹ä»£ç ï¼‰'
            logging.error(url, 'æœªèƒ½è¯†åˆ«æ ‡é¢˜')
    else:
        return 'é©å‘½å°šæœªæˆåŠŸ åŒå¿—ä»éœ€åŠªåŠ› ï¼ˆæœªèƒ½è¯†åˆ«æ ‡é¢˜ï¼Œè¯·ä¿®æ”¹ä»£ç ï¼‰'


def rillaget(url: str, folder_path: str, header: str) -> None:
    try:
        response = requests.get(url, headers=header, stream=True, timeout=30)

        # Check if response is successful
        if response.status_code == 200:

            # Figure out image real format
            content_type_name = response.headers.get('Content-Type')
            image_format = content_type_name.split('/')[-1]

            filename = ".".join([url.split("/")[-2], image_format])
            total_path = os.path.join(
                SAVE_DIRECTORY, folder_path, filename)

            # Skip existing file
            if os.path.exists(total_path):
                print(f"file already exists: {total_path}")
                return

            expected_length = int(response.headers.get('Content-Length', 0))
            current_length = 0

            with open(total_path, 'wb') as f:
                for chunk in response.iter_content(1024):
                    f.write(chunk)
                    current_length += len(chunk)

                    # Content-Length check
                    if current_length >= expected_length:
                        break

            # Use md5 to rename the file
            rename_to_sha256(total_path)

        else:
            print(
                f"Failed to download {url}. HTTP Status Code: {response.status_code}")
            logging.error(
                f"Failed to download {url}. HTTP Status Code: {response.status_code}")

    except OSError as f:
        # "Cannot create a file when that file already exists"
        print(f)
        logging.error(f)

    except Exception as e:
        print(f"Error occurred while downloading {url}:\n{str(e)}")
        logging.error(f"Error occurred while downloading {url}:\n{str(e)}")


def rename_to_sha256(filepath):
    file_name = filepath.split("\\")[-1].split(".")[0]
    new_name = get_sha256(filepath)
    new_path = filepath.replace(file_name, new_name)
    try:
        os.rename(filepath, new_path)
    except FileExistsError:
        os.remove(filepath)


def get_sha256(filepath):
    # Open,close, read file and calculate sha256 on its contents
    with open(filepath, 'rb') as file_to_check:
        # read contents of the file
        data = file_to_check.read()
        # pipe contents of the file through
        sha256_returned = hashlib.sha256(data).hexdigest()

    # Finally compare original sha256 with freshly calculated
    return sha256_returned


def webp_to_png_and_fix_gif(folder_path):
    for file in os.listdir(folder_path):
        if ".webp" in file:
            webp_path = os.path.join(folder_path, file)
            print(f"Found Webp! {webp_path}")
            logging.info(f"Found Webp! {webp_path}")
            png_path = webp_path.replace('webp', 'png')
            cmd = f'{STATIC} "{webp_path}" -o "{png_path}"'
            result = subprocess.run(
                cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)
            if result.returncode != 0:
                print("Error occurred:", result.stderr.decode())
                logging.error("Error occurred:", result.stderr.decode())
            else:
                os.remove(webp_path)
                rename_to_sha256(png_path)

        if ".gif" in file:
            gif_path = os.path.join(folder_path, file)
            temp_path = os.path.join(folder_path, 'temp.gif')
            print(f"Found Gif! {file}")
            logging.info(f"Found Gif! {file}")
            cmd = f'ffmpeg -i "{gif_path}" -y "{temp_path}"'
            result = subprocess.run(
                cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)
            if result.returncode != 0:
                print("Error occurred:", result.stderr.decode())
                logging.error("Error occurred:", result.stderr.decode())
            else:
                os.replace(temp_path, gif_path)
                rename_to_sha256(gif_path)


def delete_if_found_duplicate(folder_path, existing_file_dict):
    for file in os.listdir(folder_path):
        f_hash = file.split(".")[0]
        if f_hash in existing_file_dict:
            print(f"{existing_file_dict[f_hash]} æ›¾ç»ä¸‹è½½è¿‡ï¼Œhashå€¼æ˜¯ {f_hash}ï¼Œåˆ é™¤ä¹‹")
            logging.warning(
                f"{existing_file_dict[f_hash]} æ›¾ç»ä¸‹è½½è¿‡ï¼Œhashå€¼æ˜¯ {f_hash}ï¼Œåˆ é™¤ä¹‹")
            # åˆ é™¤ï¼ï¼ï¼ï¼
            os.remove(os.path.join(folder_path, file))
        else:
            existing_file_dict[f_hash] = os.path.join(folder_path, file)

    # å‡å¦‚åˆ é™¤å®Œæ¯•ï¼Œè¯¥æ–‡ä»¶å¤¹ä¸ºç©ºï¼Œåˆ™ç›´æ¥åˆ é™¤æ•´ä¸ªæ–‡ä»¶å¤¹
    if len(os.listdir(folder_path)) == 0:
        print(f"{folder_path} ä¸ºç©ºï¼Œåˆ é™¤ä¹‹")
        logging.warning(f"{folder_path} ä¸ºç©ºï¼Œåˆ é™¤ä¹‹")
        # åˆ é™¤ï¼ï¼ï¼ï¼
        shutil.rmtree(folder_path)


def read_json() -> dict:
    '''
    ä»DATABASEä¸­è¯»å–å­—å…¸æ–‡ä»¶
    '''
    with open(DATABSE, 'r') as f:
        return json.load(f)


def write_json(stored_dict: dict) -> None:
    with open(DATABSE, 'w') as f:
        json.dump(stored_dict, f)


def resume_from_breakpoint(existing_file_dict):
    '''
    æ–­ç‚¹ç»­ä¼ åŠŸèƒ½
    '''
    is_rfb_needed = False
    # å…ˆæ£€æµ‹ä¸€ä¸‹çœ‹çœ‹SAVE_DIRECTORYæ–‡ä»¶å¤¹é‡Œé¢æœ‰æ²¡æœ‰ä¸œè¥¿
    with os.scandir(SAVE_DIRECTORY) as it:
        if any(it):
            print("æ£€æµ‹åˆ°SAVE_DIRECTORYæ–‡ä»¶å¤¹ä¸­å­˜åœ¨æ–‡ä»¶ï¼Œå¯èƒ½æ˜¯ä¸Šæ¬¡ä»»åŠ¡ä¸­æ–­å¼•èµ·ï¼Œå°†å¯åŠ¨æ–­ç‚¹ç»­ä¼ åŠŸèƒ½")
            logging.info("æ£€æµ‹åˆ°SAVE_DIRECTORYæ–‡ä»¶å¤¹ä¸­å­˜åœ¨æ–‡ä»¶ï¼Œå¯èƒ½æ˜¯ä¸Šæ¬¡ä»»åŠ¡ä¸­æ–­å¼•èµ·ï¼Œå°†å¯åŠ¨æ–­ç‚¹ç»­ä¼ åŠŸèƒ½")
            is_rfb_needed = True

    if is_rfb_needed:
        for author, _, files in os.walk(SAVE_DIRECTORY):
            for file in files:
                file_path = os.path.join(author, file)
                f_hash = file.split(".")[0]
                existing_file_dict[f_hash] = file_path


def play():
    # è·å¾—è¦ä¸‹è½½çš„æ–‡ä»¶çš„urlåˆ—è¡¨
    URLS = get_urls_from_download_txt(ICLOUD)

    if duplicate_filter:
        # ä»å­˜æ¡£ä¸­è¯»å–æ‰€æœ‰æ–‡ä»¶çš„Hashå€¼
        print("æ­£åœ¨è¯»å–å­˜æ¡£ï¼Œè¯·ç¨å€™")
        existing_file_dict = read_json()

        # æ–­ç‚¹ç»­ä¼ åŠŸèƒ½ï¼ŒæŠŠSAVE_DIRECTORYæ–‡ä»¶å¤¹é‡Œé¢çš„æ–‡ä»¶çš„Hashå€¼ä¹Ÿæ·»åŠ åˆ°å­—å…¸ä¸­å»
        resume_from_breakpoint(existing_file_dict)

    # æ­£å¼å¼€å§‹ä¸‹è½½å·¥ä½œ
    for index, url in enumerate(URLS, start=1):

        soup, content = get_soup_from_webpage(url, HEADER, timeout=15)

        title = find_title(url, soup)

        if title == 'Weixin Official Accounts Platform':
            logging.error("ç–‘ä¼¼é‡åˆ°æœåŠ¡å™¨æŠ½é£ï¼Œç­‰å¾…5ç§’é’Ÿåé‡è¯•")
            logging.error(url)
            logging.error(title)
            print("ç–‘ä¼¼é‡åˆ°æœåŠ¡å™¨æŠ½é£ï¼Œç­‰å¾…5ç§’é’Ÿåé‡è¯•")
            time.sleep(5)
            soup, content = get_soup_from_webpage(url, HEADER, timeout=15)
            title = find_title(url, soup)

        gfw_title = [
            "The content has been deleted by the author.",
            "This account has been deleted by the owner. Unable to view the content.",
            "è¯¥å†…å®¹å·²è¢«å‘å¸ƒè€…åˆ é™¤",
            "æ­¤è´¦å·å·²è¢«å±è”½, å†…å®¹æ— æ³•æŸ¥çœ‹",
            "æ­¤å†…å®¹å› è¿è§„æ— æ³•æŸ¥çœ‹",
        ]

        if title.strip() in gfw_title:  # æ­¤å†…å®¹å› è¿è§„æ— æ³•æŸ¥çœ‹å‰åæœ‰ç©ºæ®µ
            print(f"  {index} æ¥æ™šäº†ï¼Œå†…å®¹è¢«å’Œè°äº†ã€‚ï¼ˆï¼›Â´Ğ´ï½€ï¼‰ã‚  ".center(78, "#"))
            logging.error(
                f"{title}  {url}  {index} æ¥æ™šäº†ï¼Œå†…å®¹è¢«å’Œè°äº†ã€‚ï¼ˆï¼›Â´Ğ´ï½€ï¼‰ã‚  ".center(78, "#"))
            print(title)
            print(url)
            print()
            continue

        if title == "é©å‘½å°šæœªæˆåŠŸ åŒå¿—ä»éœ€åŠªåŠ› ï¼ˆæœªèƒ½è¯†åˆ«æ ‡é¢˜ï¼Œè¯·ä¿®æ”¹ä»£ç ï¼‰":
            print(soup)
            break

        print(index, title, url)
        logging.info(index)
        logging.info(title)
        logging.info(url)

        downlist = extract_image_using_re_only(content)
        if not downlist:
            print("æ˜¯æ—¶å€™å»å’¨è¯¢ä¸€ä¸‹chatGPTäº†")
            logging.critical("æœªèƒ½å‘ç°ä»»ä½•é“¾æ¥ï¼Œæ˜¯æ—¶å€™å»å’¨è¯¢ä¸€ä¸‹chatGPTäº†")
            continue

        logging.info(downlist)

        # skip emoji
        for link in downlist[:]:
            if "/we-emoji/" in link:
                downlist.remove(link)
                continue

        # Only if downlist is not empty
        if not len(downlist):
            print("æ²¡æœ‰å‘ç°å›¾ç‰‡ï¼Œå»ºè®®æ’æ•…")
            print()
            break

        # Check if folder is there, if not, create one
        folder_path = os.path.join(
            SAVE_DIRECTORY, make_name_valid(title))

        # Ensure directory exists
        if not os.path.exists(folder_path):
            os.mkdir(folder_path)

        print(f"å…±æ‰¾åˆ°{len(downlist)}å¼ å›¾ç‰‡")
        logging.info(f"å…±æ‰¾åˆ°{len(downlist)}å¼ å›¾ç‰‡")

        threads = []
        for img_url in downlist:
            t = Thread(target=rillaget, args=[img_url, folder_path, HEADER])
            t.start()
            threads.append(t)
        for t in threads:
            t.join()

        # Check and convert WebP to PNG and also fix Gif
        webp_to_png_and_fix_gif(folder_path)

        if duplicate_filter:
            # Figure out if any file has been duplicated with database record
            delete_if_found_duplicate(folder_path, existing_file_dict)

        print()

    if duplicate_filter:
        # å°†æ–°çš„æ‰€æœ‰æ–‡ä»¶çš„Hashå€¼å†™å›åˆ°å­˜æ¡£ä¸­
        print("æ­£åœ¨å†™å…¥å­˜æ¡£ï¼Œè¯·ç¨å€™")
        write_json(existing_file_dict)


class NotUseAnymore():
    def test():
        print()

    def get_md5(filepath):
        # Open,close, read file and calculate MD5 on its contents
        with open(filepath, 'rb') as file_to_check:
            # read contents of the file
            data = file_to_check.read()
            # pipe contents of the file through
            md5_returned = hashlib.md5(data).hexdigest()

        # Finally compare original MD5 with freshly calculated
        return md5_returned

    def extract_image_url(soup: BeautifulSoup) -> list:
        raw_down_list = []
        try:
            tags = soup.find_all('img')
            for n in tags:
                raw_down_list.append(n['data-src'])
        except:
            # ä» 2023-03-03 å¼€å§‹ï¼Œå¾®ä¿¡ä½¿åï¼Œå¼€å§‹ç”¨JavaScriptæ¥å¯¹å›¾ç‰‡åœ°å€è¿›è¡Œéšè—
            hidden_section = soup.find('div', id="js_content")
            hidden_img_tags = hidden_section.find_all('img')
            for n in hidden_img_tags:
                raw_down_list.append(n['data-src'])

        # å»é‡
        downlist = list(set(raw_down_list))
        return downlist

    def extract_image_from_cdn_version_page(content: str) -> list:
        pattern = r"cdn_url:\s*'([^']*)'"

        raw_down_list = re.findall(pattern, content)
        logging.info(raw_down_list)
        cleaned_urls = [url.split("\\")[0] for url in raw_down_list]

        if raw_down_list:
            # å»é‡
            downlist = list(set(cleaned_urls))
            return downlist

        else:
            print("No image URLs found.")
            return None

    def rillatest():
        print("è®©æˆ‘ä»¬è¡èµ·åŒæ¡¨")

    def input_url():
        print('https://mp.weixin.qq.com/s/KAI2s49-pXLtPX7FudnRQg')
        url = input('è¾“å…¥è¦ä¸‹è½½çš„å¾®ä¿¡å…¬ä¼—å·çš„ç½‘å€ï¼Œæ ¼å¼å¦‚ä¸Šï¼š ')
        return url

    def try_soup_ten_times(url, header=HEADER):
        soup_attemp = 0
        success_status = False
        while soup_attemp < 10 and not success_status:
            try:
                soup, content = get_soup_from_webpage(url, header, timeout=15)
                title = find_title(url, soup)

                if title == "autorestart":
                    print(f"when opening\n{url}, error occur:")
                success_status = True

            except:
                soup_attemp += 1
                print("Seems the network is not very stable...")
                print("Let's wait for 5 seconds.")
                time.sleep(5)
                print("Ok, let's try again.")
                if soup_attemp == 10:
                    print("Well, let it go.")
                    return ("404", "404")
        return soup, content, title

    def big_pick():
        for file in os.listdir(SAVE_DIRECTORY):
            file_path = os.path.join(SAVE_DIRECTORY, file)
            with Image.open(file_path) as picture:
                image_width, image_height = picture.size
            if image_height > 1001 and image_width > 1001:
                shutil.move(file_path, r'D:\RMT\Tencent Wechat\Review')

    def cwtp(total_path):
        # Conver WebP to PNG
        if '.webp' in total_path:
            print(f"Found Webp! {repr(total_path)}")
            webp_to_png_and_fix_gif(repr(total_path))

    def find_filename(img_url: str) -> str:
        """
        2024å¹´7æœˆ25æ—¥
        å†³å®šé‡å†™ï¼ŒæŠŠåˆ¤æ–­æ ¼å¼çš„é€»è¾‘æ”¾åˆ°ä¸‹è½½å™¨é‡Œé¢å»
        """
        if 'mmbiz.qpic.cn' in img_url:
            return img_url.split("/")[-2]

    def Abandon():
        # for author, _, files in os.walk(SAVE_DIRECTORY):
        #     for file in files:
        #         file_path = os.path.join(author, file)
        #         f_hash = file.split(".")[0]
        #         existing_file_dict[f_hash] = file_path

        # # æŠŠå·²ç»ç²¾é€‰è¿‡äº†çš„æ–‡ä»¶ä¹Ÿç®—è¿›æ¥
        # for title, _, files in os.walk(r"D:\RMT\Tencent Wechat\Wechat"):
        #     for file in files:
        #         if '.txt' in file:
        #             continue
        #         if '.json' in file:
        #             continue
        #         file_path = os.path.join(title, file)
        #         f_hash = file.split(".")[0]
        #         existing_file_dict[f_hash] = file_path

        # write_json(existing_file_dict)
        pass


if __name__ == '__main__':
    # è¿‡æ»¤å™¨å¼€å…³
    duplicate_filter = True
    play()
